# vision_transformer_classifier
В данном проекте реализован трансформер (Vision Transformer) на основе предобученной модели из библиотеки timm. Модель обучена с целью идентификации личности с учетом имеющихся ограничений. В реальной ситуации трудности идентификации личности по фотографии могут создавать такие атрибуты, как медицинские маски, шляпы и другое, то есть те объекты, которые закрывают часть лица. 
---
# Датасет
Для обучения, тестирования и создания базы эмбеддингов был использован публичный датасет CelebA. 
CelebA - обширный датасет из более чем 200 000 изображений знаменитостей, где лица уже отцентрированы.
Для начала работы на гугл-диск был загружен файл `img_align_celeba.zip`. Это архив датасета, полученный с [официального источника](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). Кроме того был использован файл `identity_CelebA.txt`, который хранит соответствие каждого изображения из датасета конкретной личности.
---
# Архитектура модели
Для дообучения был использован трансформер `vit_base_patch16_224.augreg_in21k_ft_in1k` из библиотеки `timm`.
Модель принимает на вход изображения размером 224×224, разбивает их на патчи 16×16, каждый из которых преобразуется в вектор признаков и обрабатывается последовательностью из 12 слоев трансформера (трансформер-блоков). Количество выходных классов в последнем слое классификатора равно количеству личностей.
---
# Обучение
Для обучения модели был создан собственный класс аугментации данных `MaskAugmentation`, реализующий накладывание "маски". "Маска" закрывает либо верхнюю часть лица (глаза), либо нижнюю часть лица (рот, подбородок).
Для дообучения были разморожены параметры последних слоев, а именно 3 блока трансформера, слой нормализации и классификационная голова.



